# -*- coding: utf-8 -*-
"""Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S5NhyBmpJ51WX7bXpnKxgH5L4yx1_wcO
"""

import nltk
nltk.download("stopwords")

nltk.download("punkt")
nltk.download('wordnet')

import numpy as np
import pandas as pd

df=pd.read_csv('/content/drive/My Drive/dataset.csv')

df.body.fillna('', inplace=True)

from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer,LancasterStemmer
from nltk.corpus import stopwords
import re
lemmatizer = WordNetLemmatizer()
stemmer = LancasterStemmer() 
def preprocess(sentence):
    sentence=str(sentence)
    sentence = sentence.lower() # Converting to lower case
    sentence=sentence.replace('{html}',"") 
    sentence=sentence.replace('\n',"") 
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', sentence)
    rem_url=re.sub(r'http\S+', '',cleantext)
    rem_num = re.sub('[0-9]+', '', rem_url)
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(rem_num)  
    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]
    stem_words=[stemmer.stem(w) for w in filtered_words]
    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]
    return " ".join(lemma_words)

df['cleanbody']=df['body'].map(lambda s:preprocess(s)) 
df['cleantitle']=df['title'].map(lambda s:preprocess(s))

df.to_csv('dataset.csv',index=False)

